{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np            \n",
    "import pandas as pd        \n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "#Đường dẫn thư mục train và test\n",
    "train_path = r'.\\train'\n",
    "test_path = r'.\\test'\n",
    "val_path = r'.\\val'\n",
    "\n",
    "batch_size = 16 \n",
    "\n",
    "img_height = 500\n",
    "img_width = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_data_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1943 images belonging to 2 classes.\n",
      "Found 611 images belonging to 2 classes.\n",
      "Found 497 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = image_gen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    color_mode='rgb'\n",
    ")\n",
    "\n",
    "test = test_data_gen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode='rgb', \n",
    "    shuffle=False,\n",
    "    class_mode='binary',\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validation_generator = image_gen.flow_from_directory(\n",
    "    val_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    color_mode='rgb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 3us/step\n",
      "{0: 1.763157894736842, 1: 1.763157894736842}\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1653s\u001b[0m 13s/step - accuracy: 0.6155 - loss: 2.8737 - val_accuracy: 0.7163 - val_loss: 0.6004 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m906s\u001b[0m 7s/step - accuracy: 0.6615 - loss: 1.2352 - val_accuracy: 0.7344 - val_loss: 0.5616 - learning_rate: 0.0010\n",
      "Epoch 3/25\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m964s\u001b[0m 8s/step - accuracy: 0.6521 - loss: 1.1410 - val_accuracy: 0.6881 - val_loss: 0.6151 - learning_rate: 0.0010\n",
      "Epoch 4/25\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.7186 - loss: 1.0334\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m800s\u001b[0m 7s/step - accuracy: 0.7187 - loss: 1.0332 - val_accuracy: 0.7203 - val_loss: 0.6031 - learning_rate: 0.0010\n",
      "Epoch 5/25\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m800s\u001b[0m 7s/step - accuracy: 0.7633 - loss: 0.9229 - val_accuracy: 0.7445 - val_loss: 0.5457 - learning_rate: 3.0000e-04\n",
      "Epoch 6/25\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m800s\u001b[0m 7s/step - accuracy: 0.7751 - loss: 0.8557 - val_accuracy: 0.7324 - val_loss: 0.5498 - learning_rate: 3.0000e-04\n",
      "Epoch 7/25\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.7521 - loss: 0.9054\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m798s\u001b[0m 7s/step - accuracy: 0.7522 - loss: 0.9051 - val_accuracy: 0.7304 - val_loss: 0.5693 - learning_rate: 3.0000e-04\n",
      "Epoch 8/25\n",
      "\u001b[1m122/122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m797s\u001b[0m 7s/step - accuracy: 0.7916 - loss: 0.7801 - val_accuracy: 0.7384 - val_loss: 0.6060 - learning_rate: 9.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ee2d7d7b60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = VGG16(include_top=False, input_shape=(img_height, img_width, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top of the pre-trained model\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the model\n",
    "cnn = Model(inputs=base_model.input, outputs=predictions)\n",
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.3, min_lr=0.000001)\n",
    "callbacks_list = [early, learning_rate_reduction]\n",
    "\n",
    "# Compute class weights\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "unique_classes = np.unique(train_generator.classes)\n",
    "weights = compute_sample_weight(class_weight='balanced', y=train_generator.classes)\n",
    "cw = dict(zip(unique_classes, weights))\n",
    "print(cw)\n",
    "\n",
    "# Train the model\n",
    "cnn.fit(\n",
    "    train_generator,\n",
    "    epochs=25,\n",
    "    validation_data=validation_generator,\n",
    "    class_weight=cw,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "cnn.save('vgg16_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 5s/step - accuracy: 0.3911 - loss: 1.2512\n",
      "The testing accuracy is: 70.54010033607483 %\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 5s/step\n",
      "Confusion Matrix:\n",
      "[[ 19 178]\n",
      " [  2 412]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.90      0.10      0.17       197\n",
      "     Disease       0.70      1.00      0.82       414\n",
      "\n",
      "    accuracy                           0.71       611\n",
      "   macro avg       0.80      0.55      0.50       611\n",
      "weighted avg       0.76      0.71      0.61       611\n",
      "\n",
      "Sensitivity: 99.52%\n",
      "Specificity: 9.64%\n",
      "F1-Score: 0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_accu = cnn.evaluate(test)\n",
    "print('The testing accuracy is:', test_accu[1] * 100, '%')\n",
    "\n",
    "# Predict the labels for the test data\n",
    "y_pred = cnn.predict(test)\n",
    "y_pred = np.round(y_pred).astype(int)  # Chuyển đổi dự đoán thành nhãn nhị phân (0 hoặc 1)\n",
    "\n",
    "# Get the true labels\n",
    "y_true = test.classes\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "# Compute classification report\n",
    "class_report = classification_report(y_true, y_pred, target_names=['Normal', 'Disease'])\n",
    "print('Classification Report:')\n",
    "print(class_report)\n",
    "\n",
    "# Extract Sensitivity, Specificity, and F1-Score from confusion matrix\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "precision = tp / (tp + fp)\n",
    "recall = sensitivity  # Recall là tên khác của Sensitivity\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f'Sensitivity: {sensitivity * 100:.2f}%')\n",
    "print(f'Specificity: {specificity * 100:.2f}%')\n",
    "print(f'F1-Score: {f1_score:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
